{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eIRhc7drd8AY"
   },
   "source": [
    "# 50.040 Natural Language Processing (Fall 2024) Final Project (100 Points)\n",
    "\n",
    "**DUE DATE: 13 December 2024**\n",
    "\n",
    "Final project will be graded by Chen Huang"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FsCU5Q_6d8Ac"
   },
   "source": [
    "\n",
    "# Group Information (Fill before you start)\n",
    "\n",
    "**Group Name: ChatGPT Course**\n",
    "\n",
    "**Name(STUDNET ID) (2-3 person):**\n",
    "\n",
    "**Beckham Wee Yu Zheng(1006010)**\n",
    "\n",
    "**Deshpande Sunny Nitin(1006336)**\n",
    "\n",
    "**Patrick Mo(1006084)**\n",
    "\n",
    "**Please also rename the final submitted pdf as ``finalproject_[GROUP_NAME].pdf``**\n",
    "\n",
    "**-1 points if info not filled or file name not adjusted before submission, -100 points if you copy other's answer. We encourage discussion, but please do not copy without thinking.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Design Challenge (30 points)\n",
    "Now it's time to come up with your own model! You are free to decide what model you want to choose or what architecture you think is better for this task. You are also free to set all the hyperparameter for training (do consider the overfitting issue and the computing cost). From here we will see how important choosing/designing a better model architecture could be when building an NLP application in practice (however, there is no requirement for you to include such comparisons in your report).\n",
    "\n",
    "You are allowed to use external packages for this challenge, but we require that you fully understand the methods/techniques that you used, and you need to clearly explain such details in the submitted report. We will evaluate your system's performance on the held-out test set, which will only be released 48 hours before the deadline. Use your new system to generate the outputs. The system that achieves the highest F1 score will be announced as the winner for the challenge. We may perform further analysis/evaluations in case there is no clear winner based on the released test set.\n",
    "\n",
    "Let's summarize this challenge:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(i) **[Model]** You are required to develop your own model for sentiment analysis, with no restrictions on the model architecture. You may choose to follow RNN/CNN structures or experiment with Transformer-based models. In your submitted report, you must provide a detailed explanation of your model along with the accompanying code. Additionally, you are required to submit your model so that we can reproduce your results.\n",
    "\n",
    "*_(10 points)_*\n",
    "\n",
    "(ii) **[Evaluation]** For a fair comparison, you must train your new model on the same dataset provided to you. After training, evaluate your model on the test set. You are required to report the **precision**, **recall**, **F1 scores**, and **accuracy** of your new model. Save the predicted outcome for the test set and include it in the submission.\n",
    "\n",
    "_Hint:_ You will be competing with other groups on the same test set. Groups with higher performance will receive more points. For instance, if your group ranks 1st among all groups, you will receive 15 points for this section.\n",
    "\n",
    "*_(15 points)_*\n",
    "\n",
    "(iii) **[Report]** You are required to submit a report for your model. The report must include, at a minimum, the following sections: Model Description, Training Settings (e.g., dataset, hyperparameters), Performance, Code to run the model, and a breakdown of how the work was divided among team members. You are encouraged to include any additional details you deem important. Instructions on how to run the code can either be included in a separate README file or integrated into the report, as long as they are clearly presented.\n",
    "\n",
    "Please provide a thorough explanation of the model or method you used for designing the new system, along with clear documentation on how to execute the code. We will review your code, and may request an interview if we have any questions about it.\n",
    "\n",
    "_Note:_ Reports, code, and README files that are of low quality or poorly written will be penalized, so please ensure they are well-organized and clearly formatted. If we are unable to reproduce your model or run your code, you will not receive any points for this challenge.\n",
    "\n",
    "*_(5 points)_*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Start your model in a different .py file with a README explaination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Installed kernelspec nlp in /Users/beckhamwee/Library/Jupyter/kernels/nlp\n"
     ]
    }
   ],
   "source": [
    "!python -m ipykernel install --user --name=nlp --display-name \"Python (nlp)\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sa oot sopo\n"
     ]
    }
   ],
   "source": [
    "print('sa oot sopo')\n",
    "import os\n",
    "import torch\n",
    "from torch import nn\n",
    "from d2l import torch as d2l # You can skip this if you have trouble with this package, all d2l-related codes can be replaced by torch functions.\n",
    "import transformers\n",
    "from transformers import DistilBertTokenizer, DistilBertModel\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import zipfile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = 'data/aclImdb'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing\n",
    "For transformers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "G0vxL-WpCdXX"
   },
   "source": [
    "### `read_imdb` From Before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TOyoT-QiCdXY"
   },
   "outputs": [],
   "source": [
    "#@save\n",
    "def read_imdb(data_dir, is_train):\n",
    "    \"\"\"Read the IMDb review dataset text sequences and labels.\"\"\"\n",
    "    ### YOUR CODE HERE\n",
    "    data, labels = [], []\n",
    "\n",
    "    for label in ('pos', 'neg'): # label is either 'pos' or 'neg'\n",
    "        folder_name = os.path.join(data_dir, 'train' if is_train else 'test', label) # folder_name is either 'train/pos' or 'train/neg' or 'test/pos' or 'test/neg'\n",
    "\n",
    "        for file in os.listdir(folder_name):\n",
    "            with open(os.path.join(folder_name, file), 'rb') as f:\n",
    "                review = f.read().decode('utf-8').replace('\\n', '')\n",
    "                data.append(review)\n",
    "                labels.append(1 if label == 'pos' else 0)\n",
    "\n",
    "    ### END OF YOUR CODE\n",
    "    return data, labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters for Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" HYPERPARAMS \"\"\"\n",
    "num_steps = 500  # sequence length\n",
    "batch_size = 16 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PVpoDgU7CdXb"
   },
   "source": [
    "### Modified Load Data Function\n",
    "Modified `load_data_imdb` function to preprocess data for DistilBERT.\n",
    "- uses `read_imdb` to read training and test data\n",
    "- Loads pretrained Tokenzier for Bert which converts raw text into tokenized sequences. Then encodes with input IDs and attention masks.\n",
    "- Truncates and pads them\n",
    "- Also converts that + labels into PyTorch Tensors\n",
    "- Returns input_ids, attention mask and testing labels\n",
    "- Creates PyTorch DataLoaders too (refer to docstrings i wrote for all outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data_imdb_hybrid(batch_size, num_steps=500, device=None):\n",
    "    \"\"\"\n",
    "    Prepares IMDb data for a hybrid Transformer + BiLSTM model.\n",
    "    Tokenizes, truncates/pads sequences, and creates DataLoaders.\n",
    "\n",
    "    Args:\n",
    "        batch_size (int): Batch size for DataLoader.\n",
    "        num_steps (int): Maximum length for truncation/padding.\n",
    "        device (torch.device): Device to move tensors to ('cpu', 'cuda', or 'mps').\n",
    "\n",
    "    Returns:\n",
    "        train_loader (tuple[tensors]): (input_ids, attension_mask, labels)\n",
    "        test_loader (tuple[tensors]): (input_ids, attension_mask, labels)\n",
    "        tokenizer (DistilBertTokenzier): Tokenizer \n",
    "    \"\"\"\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # Load IMDb dataset\n",
    "    train_data = read_imdb(data_dir, is_train=True)\n",
    "    test_data = read_imdb(data_dir, is_train=False)n\n",
    "\n",
    "    # Load pre-trained tokenizer\n",
    "    tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "    \n",
    "    # Tokenize and encode sequences\n",
    "    def preprocess(data):\n",
    "        return tokenizer(\n",
    "            data,\n",
    "            truncation=True,\n",
    "            padding=\"max_length\",\n",
    "            max_length=num_steps,\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "    \n",
    "    train_tokens = preprocess(train_data[0])\n",
    "    test_tokens = preprocess(test_data[0])\n",
    "\n",
    "    # Move tokenized data to device\n",
    "    train_tokens = {key: val.to(device) for key, val in train_tokens.items()}\n",
    "    test_tokens = {key: val.to(device) for key, val in test_tokens.items()}\n",
    "    \n",
    "    # Convert labels to tensors and move to device\n",
    "    train_labels = torch.tensor(train_data[1]).to(device)\n",
    "    test_labels = torch.tensor(test_data[1]).to(device)\n",
    "\n",
    "    # Create TensorDatasets\n",
    "    train_dataset = TensorDataset(\n",
    "        train_tokens[\"input_ids\"], train_tokens[\"attention_mask\"], train_labels\n",
    "    )\n",
    "    test_dataset = TensorDataset(\n",
    "        test_tokens[\"input_ids\"], test_tokens[\"attention_mask\"], test_labels\n",
    "    )\n",
    "    \n",
    "    # Create DataLoaders\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "    \n",
    "    return train_loader, test_loader, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb3c311a160f431fbdf3a475ab981b1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76df366df9f24f8c86018ec5389de553",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5930dda0759c469fa66ed845a49c770b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/466k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "936516e920b84d57889e9cbe7dfc286a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load data\n",
    "# TODO: Optimise batch size (?)\n",
    "train_loader, test_loader, tokenizer = load_data_imdb_hybrid(16, num_steps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2kAezpJ9CdXd"
   },
   "source": [
    "# Building the Transformer-BiLSTM Model for Sentiment Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Choice of Model: DistilBERT + BiLSTM\n",
    "- DistilBERT to capture long-range dependencies of works using the self-attention mechanism (can see context from all other tokens in its input sequence)\n",
    "- BiLSTMS to capture temporal sequenctial local features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building the Model\n",
    "DistilBERT to capture contextualised word emebeddings from input text. Handles nunances in meaning, context and r/s btw words\n",
    "- semantic and syntactic r/s between words using pretrained knowledge\n",
    "- takes input_ids and attentions_masks, outputs last hidden state of shape `(batch_size, seq_length, hidden_size)`\n",
    "\n",
    "BiLSTM enhances the embeddings above by modelling sequential relationshpis (order of words, temporal patterns etc)\n",
    "- adds sequential modelling\n",
    "- takes last state from transformer and outputs contextualised embeddings of sequential relationships\n",
    "\n",
    "Fully connected layer (last) converts the high d output of LSTM into class prbabilities to make classification predictions\n",
    "- passed through dropout layer to regularize and prevent verfitting\n",
    "- applies linear transformation to map features to number of classes\n",
    "- outputs logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "train_iter, test_iter, vocab = load_data_imdb_hybrid(batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TransformerBiLSTM(nn.Module):\n",
    "    def __init__(self, transformer_model=\"distilbert-base-uncased\", hidden_size=128, num_classes=2, dropout=0.5):\n",
    "        super(TransformerBiLSTM, self).__init__()\n",
    "        self.transformer = DistilBertModel.from_pretrained(transformer_model) # Pre-trained DistilBERT\n",
    "        print(\"Model loaded successfully!\")\n",
    "        \n",
    "        # BiLSTM layer\n",
    "        self.bilstm = nn.LSTM(\n",
    "            input_size=self.transformer.config.hidden_size,  # Size of Transformer embeddings\n",
    "            hidden_size=hidden_size,\n",
    "            num_layers=1,\n",
    "            bidirectional=True,\n",
    "            batch_first=True\n",
    "        )\n",
    "        \n",
    "        # Fully connected output layer\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)  # BiLSTM is bidirectional\n",
    "        \n",
    "        # Dropout for regularization\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, input_ids, attention_mask):\n",
    "        # Pass through Transformer\n",
    "        transformer_output = self.transformer(input_ids, attention_mask=attention_mask)\n",
    "        x = transformer_output.last_hidden_state  # Shape: (batch_size, seq_len, hidden_size)\n",
    "        \n",
    "        # Pass through BiLSTM\n",
    "        lstm_out, _ = self.bilstm(x)  # Shape: (batch_size, seq_len, hidden_size*2)\n",
    "        lstm_out = lstm_out[:, -1, :]  # Get the last hidden state for classification\n",
    "        \n",
    "        # Fully connected layer\n",
    "        x = self.dropout(lstm_out)\n",
    "        logits = self.fc(x)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let’s construct a bidirectional RNN with two hidden layers to represent single text for sentiment analysis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import AdamW\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from transformers import get_scheduler\n",
    "\n",
    "def train_model(\n",
    "    model, train_loader, test_loader, num_epochs=3, lr=2e-5, device=None, save_model=True, save_dir=\"models\"\n",
    "):\n",
    "    \"\"\"\n",
    "    Train the model, plot metrics, and optionally save the model.\n",
    "\n",
    "    Args:\n",
    "        model: The model to train.\n",
    "        train_loader: DataLoader for training data.\n",
    "        test_loader: DataLoader for testing data.\n",
    "        num_epochs: Number of epochs.\n",
    "        lr: Learning rate.\n",
    "        device: Device for training ('cuda', 'mps', or 'cpu').\n",
    "        save_model: Whether to save the model after each epoch.\n",
    "        save_dir: Directory to save the model checkpoints.\n",
    "\n",
    "    Returns:\n",
    "        A dictionary containing metrics: train_loss, train_accuracy, test_accuracy.\n",
    "    \"\"\"\n",
    "    # Move model to device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 'cuda' for Nvidia GPUs, 'mps' for Apple Silicon\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    criterion = CrossEntropyLoss()\n",
    "\n",
    "    # Scheduler for learning rate decay\n",
    "    num_training_steps = len(train_loader) * num_epochs\n",
    "    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=num_training_steps)\n",
    "\n",
    "    # Metrics tracking\n",
    "    train_losses = []\n",
    "    train_accuracies = []\n",
    "    test_accuracies = []\n",
    "\n",
    "    # Create directory to save models\n",
    "    if save_model:\n",
    "        Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Best test accuracy for saving the best model\n",
    "    best_test_accuracy = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Starting epoch {epoch + 1}/{num_epochs}\")\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "        batch_count = 1\n",
    "\n",
    "        for batch in train_loader:\n",
    "            if batch_count % 100 == 0 or batch_count == 1:\n",
    "                print(f\"Processing batch {batch_count}/{len(train_loader)}\")\n",
    "\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "            # Update batch count\n",
    "            batch_count += 1\n",
    "\n",
    "        # Calculate average loss and accuracy for training\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = total_correct / len(train_loader.dataset)\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {avg_loss:.4f}, Train Acc: {train_accuracy:.4f}\")\n",
    "\n",
    "        # Evaluate on the test set and track test accuracy\n",
    "        test_accuracy = evaluate_model_with_metrics(model, test_loader, device)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        # Save model after each epoch or when test accuracy improves\n",
    "        if save_model:\n",
    "            model_path = os.path.join(save_dir, f\"model_epoch_{epoch+1}.pt\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Model saved to {model_path}\")\n",
    "\n",
    "            # Save the best model based on test accuracy\n",
    "            if test_accuracy > best_test_accuracy:\n",
    "                best_test_accuracy = test_accuracy\n",
    "                best_model_path = os.path.join(save_dir, \"best_model.pt\")\n",
    "                torch.save(model.state_dict(), best_model_path)\n",
    "                print(f\"Best model updated and saved to {best_model_path}\")\n",
    "\n",
    "    # Plot the results\n",
    "    plot_training_metrics(train_losses, train_accuracies, test_accuracies, num_epochs)\n",
    "\n",
    "    return {\"train_losses\": train_losses, \"train_accuracies\": train_accuracies, \"test_accuracies\": test_accuracies}\n",
    "\n",
    "\n",
    "def evaluate_model_with_metrics(model, data_loader, device):\n",
    "    \"\"\"\n",
    "    Evaluate the model and calculate accuracy on the test set.\n",
    "\n",
    "    Args:\n",
    "        model: The trained model.\n",
    "        data_loader: DataLoader for the test set.\n",
    "        device: Device for evaluation.\n",
    "\n",
    "    Returns:\n",
    "        Test accuracy.\n",
    "    \"\"\"\n",
    "    model.eval()\n",
    "    total_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for batch in data_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "\n",
    "    accuracy = total_correct / len(data_loader.dataset)\n",
    "    print(f\"Test Accuracy: {accuracy:.4f}\")\n",
    "    return accuracy\n",
    "\n",
    "\n",
    "def plot_training_metrics(train_losses, train_accuracies, test_accuracies, num_epochs):\n",
    "    \"\"\"\n",
    "    Plot training loss, training accuracy, and test accuracy per epoch.\n",
    "\n",
    "    Args:\n",
    "        train_losses: List of training losses per epoch.\n",
    "        train_accuracies: List of training accuracies per epoch.\n",
    "        test_accuracies: List of test accuracies per epoch.\n",
    "        num_epochs: Number of epochs.\n",
    "    \"\"\"\n",
    "    epochs = range(1, num_epochs + 1)\n",
    "\n",
    "    plt.figure(figsize=(12, 6))\n",
    "\n",
    "    # Plot training loss\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.plot(epochs, train_losses, label=\"Train Loss\", marker=\"o\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Loss\")\n",
    "    plt.title(\"Training Loss\")\n",
    "    plt.legend()\n",
    "\n",
    "    # Plot training and test accuracy\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.plot(epochs, train_accuracies, label=\"Train Accuracy\", marker=\"o\")\n",
    "    plt.plot(epochs, test_accuracies, label=\"Test Accuracy\", marker=\"o\", linestyle=\"--\")\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Train vs. Test Accuracy\")\n",
    "    plt.legend()\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model loaded successfully!\n",
      "Starting epoch 1/3\n",
      "Processing batch 1/1563\n",
      "Processing batch 100/1563\n",
      "Processing batch 200/1563\n",
      "Processing batch 300/1563\n",
      "Processing batch 400/1563\n",
      "Processing batch 500/1563\n",
      "Processing batch 600/1563\n",
      "Processing batch 700/1563\n",
      "Processing batch 800/1563\n",
      "Processing batch 900/1563\n",
      "Processing batch 1000/1563\n",
      "Processing batch 1100/1563\n",
      "Processing batch 1200/1563\n",
      "Processing batch 1300/1563\n",
      "Processing batch 1400/1563\n",
      "Processing batch 1500/1563\n",
      "Epoch 1/3, Loss: 0.2775, Accuracy: 0.8822\n",
      "Test Accuracy: 0.9199\n",
      "Starting epoch 2/3\n",
      "Processing batch 1/1563\n",
      "Processing batch 100/1563\n",
      "Processing batch 200/1563\n",
      "Processing batch 300/1563\n",
      "Processing batch 400/1563\n",
      "Processing batch 500/1563\n",
      "Processing batch 600/1563\n",
      "Processing batch 700/1563\n",
      "Processing batch 800/1563\n",
      "Processing batch 900/1563\n",
      "Processing batch 1000/1563\n",
      "Processing batch 1100/1563\n",
      "Processing batch 1200/1563\n",
      "Processing batch 1300/1563\n",
      "Processing batch 1400/1563\n",
      "Processing batch 1500/1563\n",
      "Epoch 2/3, Loss: 0.1443, Accuracy: 0.9491\n",
      "Test Accuracy: 0.9280\n",
      "Starting epoch 3/3\n",
      "Processing batch 1/1563\n",
      "Processing batch 100/1563\n",
      "Processing batch 200/1563\n",
      "Processing batch 300/1563\n",
      "Processing batch 400/1563\n",
      "Processing batch 500/1563\n",
      "Processing batch 600/1563\n",
      "Processing batch 700/1563\n",
      "Processing batch 800/1563\n",
      "Processing batch 900/1563\n",
      "Processing batch 1000/1563\n",
      "Processing batch 1100/1563\n",
      "Processing batch 1200/1563\n",
      "Processing batch 1300/1563\n",
      "Processing batch 1400/1563\n",
      "Processing batch 1500/1563\n",
      "Epoch 3/3, Loss: 0.0727, Accuracy: 0.9779\n",
      "Test Accuracy: 0.9290\n"
     ]
    }
   ],
   "source": [
    "# Initialize the model\n",
    "model = TransformerBiLSTM()\n",
    "\n",
    "# Train and evaluate\n",
    "train_model(model, train_loader, test_loader, num_epochs=3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning\n",
    "Learning Rate `lr`, Sequence Length `num_steps`, Dropout. Due to a lack of time, will not be tuning number of epochs.\n",
    "\n",
    "Will start by seeing which `lr` has the best results for 1 epoch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model results for learning rate 1e-05\n",
      "Starting epoch 1/1\n",
      "Processing batch 1/1563\n",
      "Processing batch 100/1563\n",
      "Processing batch 200/1563\n",
      "Processing batch 300/1563\n",
      "Processing batch 400/1563\n",
      "Processing batch 500/1563\n",
      "Processing batch 600/1563\n",
      "Processing batch 700/1563\n",
      "Processing batch 800/1563\n",
      "Processing batch 900/1563\n",
      "Processing batch 1000/1563\n",
      "Processing batch 1100/1563\n",
      "Processing batch 1200/1563\n",
      "Processing batch 1300/1563\n",
      "Processing batch 1400/1563\n",
      "Processing batch 1500/1563\n",
      "Epoch 1/1, Loss: 0.0609, Accuracy: 0.9824\n",
      "Test Accuracy: 0.9288\n",
      "Model results for learning rate 3e-05\n",
      "Starting epoch 1/1\n",
      "Processing batch 1/1563\n",
      "Processing batch 100/1563\n",
      "Processing batch 200/1563\n",
      "Processing batch 300/1563\n",
      "Processing batch 400/1563\n",
      "Processing batch 500/1563\n",
      "Processing batch 600/1563\n",
      "Processing batch 700/1563\n",
      "Processing batch 800/1563\n",
      "Processing batch 900/1563\n",
      "Processing batch 1000/1563\n",
      "Processing batch 1100/1563\n",
      "Processing batch 1200/1563\n",
      "Processing batch 1300/1563\n",
      "Processing batch 1400/1563\n",
      "Processing batch 1500/1563\n",
      "Epoch 1/1, Loss: 0.0755, Accuracy: 0.9761\n",
      "Test Accuracy: 0.9274\n",
      "Model results for learning rate 5e-05\n",
      "Starting epoch 1/1\n",
      "Processing batch 1/1563\n",
      "Processing batch 100/1563\n",
      "Processing batch 200/1563\n",
      "Processing batch 300/1563\n",
      "Processing batch 400/1563\n",
      "Processing batch 500/1563\n",
      "Processing batch 600/1563\n",
      "Processing batch 700/1563\n",
      "Processing batch 800/1563\n",
      "Processing batch 900/1563\n",
      "Processing batch 1000/1563\n",
      "Processing batch 1100/1563\n",
      "Processing batch 1200/1563\n",
      "Processing batch 1300/1563\n",
      "Processing batch 1400/1563\n",
      "Processing batch 1500/1563\n",
      "Epoch 1/1, Loss: 0.0813, Accuracy: 0.9733\n",
      "Test Accuracy: 0.9314\n"
     ]
    }
   ],
   "source": [
    "lr_values = [1e-5, 3e-5, 5e-5]\n",
    "\n",
    "for lr in lr_values:\n",
    "    print(f'Model results for learning rate {lr}') \n",
    "    train_model(model, train_loader, test_loader, num_epochs=1, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Judging from this, `lr = 5e-05` seems to provide the best results on our test set (cross-validation set). Moving on, we fix the learning rate and now try different dropout values. To do so, we create a second training function that includes dropout modifications, as well as graphs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "def train_with_dropout(model_class, train_loader, test_loader, num_epochs, lr, dropout, device, save_dir=\"models\"):\n",
    "    \"\"\"\n",
    "    Train the model with a specific dropout value and track metrics, saving the model during training.\n",
    "\n",
    "    Args:\n",
    "        model_class: The model class to instantiate (e.g., TransformerBiLSTM).\n",
    "        train_loader: DataLoader for training data.\n",
    "        test_loader: DataLoader for testing data.\n",
    "        num_epochs: Number of epochs to train.\n",
    "        lr: Learning rate.\n",
    "        dropout: Dropout rate to apply in the model.\n",
    "        device: Device for training (e.g., 'cuda', 'mps', or 'cpu').\n",
    "        save_dir: Directory to save the model checkpoints.\n",
    "\n",
    "    Returns:\n",
    "        metrics: Dictionary containing train losses, train accuracies, and test accuracies.\n",
    "    \"\"\"\n",
    "\n",
    "    \n",
    "    # Instantiate the model with the specified dropout\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\") # cuda for windows, mps for apple silicon\n",
    "    model = model_class(dropout=dropout).to(device)\n",
    "\n",
    "    # Optimizer and loss function\n",
    "    optimizer = AdamW(model.parameters(), lr=lr)\n",
    "    criterion = CrossEntropyLoss()\n",
    "    scheduler = get_scheduler(\"linear\", optimizer=optimizer, num_warmup_steps=0, num_training_steps=len(train_loader) * num_epochs)\n",
    "\n",
    "    # Prepare directory to save models\n",
    "    Path(save_dir).mkdir(parents=True, exist_ok=True)\n",
    "    best_test_accuracy = 0  # Track the best test accuracy for saving the best model\n",
    "\n",
    "    train_losses, train_accuracies, test_accuracies = [], [], []\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs}, Dropout: {dropout}\")\n",
    "        model.train()\n",
    "        total_loss, total_correct = 0, 0\n",
    "\n",
    "        # track bactch\n",
    "        batch_count = 1\n",
    "        \n",
    "        for batch in train_loader:\n",
    "            if batch_count % 100 == 0 or batch_count == 1:\n",
    "                print(f\"Processing batch {batch_count}/{len(train_loader)}\")\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            loss = criterion(outputs, labels)\n",
    "\n",
    "            # Backward pass and optimization\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            scheduler.step()\n",
    "\n",
    "            # Track metrics\n",
    "            total_loss += loss.item()\n",
    "            total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "            batch_count += 1\n",
    "\n",
    "        # Calculate average loss and accuracy for training\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        train_accuracy = total_correct / len(train_loader.dataset)\n",
    "        train_losses.append(avg_loss)\n",
    "        train_accuracies.append(train_accuracy)\n",
    "\n",
    "        # Evaluate on the test set\n",
    "        model.eval()\n",
    "        total_correct = 0\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                input_ids, attention_mask, labels = batch\n",
    "                input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "                outputs = model(input_ids, attention_mask)\n",
    "                total_correct += (outputs.argmax(dim=1) == labels).sum().item()\n",
    "        test_accuracy = total_correct / len(test_loader.dataset)\n",
    "        test_accuracies.append(test_accuracy)\n",
    "\n",
    "        print(f\"Epoch {epoch + 1}/{num_epochs} -> Train Loss: {avg_loss:.4f}, Train Acc: {train_accuracy:.4f}, Test Acc: {test_accuracy:.4f}\")\n",
    "\n",
    "        # Save model checkpoint if test accuracy improves\n",
    "        if test_accuracy > best_test_accuracy:\n",
    "            best_test_accuracy = test_accuracy\n",
    "            model_path = os.path.join(save_dir, f\"model_dropout_{dropout}_epoch_{epoch + 1}.pt\")\n",
    "            torch.save(model.state_dict(), model_path)\n",
    "            print(f\"Saved model to {model_path}\")\n",
    "\n",
    "    return {\n",
    "        \"train_losses\": train_losses,\n",
    "        \"train_accuracies\": train_accuracies,\n",
    "        \"test_accuracies\": test_accuracies,\n",
    "    }\n",
    "\n",
    "def plot_loss_across_dropouts(metrics_dict):\n",
    "    \"\"\"\n",
    "    Plot training loss across different dropout values.\n",
    "\n",
    "    Args:\n",
    "        metrics_dict: Dictionary where keys are dropout values, and values are metrics from training.\n",
    "    \"\"\"\n",
    "    fig, ax1 = plt.subplots(figsize=(10, 6))\n",
    "\n",
    "    # Extract training loss and test accuracy for each dropout\n",
    "    dropouts = []\n",
    "    losses = []\n",
    "    accuracies = []\n",
    "    for dropout, metrics in metrics_dict.items():\n",
    "        dropouts.append(dropout)\n",
    "        # Since we're running for 1 epoch only, take the first (and only) training loss and accuracy\n",
    "        losses.append(metrics[\"train_losses\"][0])\n",
    "        accuracies.append(metrics[\"test_accuracies\"][0])\n",
    "\n",
    "    # Plot training loss\n",
    "    ax1.plot(dropouts, losses, marker='o', color='blue', label=\"Training Loss\")\n",
    "    ax1.set_xlabel(\"Dropout Rate\")\n",
    "    ax1.set_ylabel(\"Loss\", color='blue')\n",
    "    ax1.tick_params(axis='y', labelcolor='blue')\n",
    "    ax1.set_title(\"Training Loss and Test Accuracy vs. Dropout Rate\")\n",
    "    \n",
    "    # Plot test accuracy on a secondary y-axis\n",
    "    ax2 = ax1.twinx()\n",
    "    ax2.plot(dropouts, accuracies, marker='o', color='green', label=\"Test Accuracy\")\n",
    "    ax2.set_ylabel(\"Accuracy (%)\", color='green')\n",
    "    ax2.tick_params(axis='y', labelcolor='green')\n",
    "\n",
    "    # Add legends\n",
    "    ax1.legend(loc=\"upper left\")\n",
    "    ax2.legend(loc=\"upper right\")\n",
    "\n",
    "    plt.grid()\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training with lr=5e-05, dropout=0.3\n",
      "Model loaded successfully!\n",
      "Epoch 1/1, Dropout: 0.3\n",
      "Processing batch 1/1563\n",
      "Processing batch 100/1563\n",
      "Processing batch 200/1563\n",
      "Processing batch 300/1563\n",
      "Processing batch 400/1563\n",
      "Processing batch 500/1563\n",
      "Processing batch 600/1563\n",
      "Processing batch 700/1563\n",
      "Processing batch 800/1563\n",
      "Processing batch 900/1563\n",
      "Processing batch 1000/1563\n",
      "Processing batch 1100/1563\n",
      "Processing batch 1200/1563\n",
      "Processing batch 1300/1563\n",
      "Processing batch 1400/1563\n",
      "Processing batch 1500/1563\n",
      "Epoch 1/1 -> Train Loss: 0.2520, Train Acc: 0.8968, Test Acc: 0.9302\n",
      "Saved model to models/model_dropout_0.3_epoch_1.pt\n",
      "Training with lr=5e-05, dropout=0.5\n",
      "Model loaded successfully!\n",
      "Epoch 1/1, Dropout: 0.5\n",
      "Processing batch 1/1563\n",
      "Processing batch 100/1563\n",
      "Processing batch 200/1563\n",
      "Processing batch 300/1563\n",
      "Processing batch 400/1563\n",
      "Processing batch 500/1563\n",
      "Processing batch 600/1563\n",
      "Processing batch 700/1563\n",
      "Processing batch 800/1563\n",
      "Processing batch 900/1563\n",
      "Processing batch 1000/1563\n",
      "Processing batch 1100/1563\n",
      "Processing batch 1200/1563\n",
      "Processing batch 1300/1563\n",
      "Processing batch 1400/1563\n",
      "Processing batch 1500/1563\n",
      "Epoch 1/1 -> Train Loss: 0.2655, Train Acc: 0.8884, Test Acc: 0.9260\n",
      "Saved model to models/model_dropout_0.5_epoch_1.pt\n",
      "Training with lr=5e-05, dropout=0.7\n",
      "Model loaded successfully!\n",
      "Epoch 1/1, Dropout: 0.7\n",
      "Processing batch 1/1563\n",
      "Processing batch 100/1563\n",
      "Processing batch 200/1563\n",
      "Processing batch 300/1563\n",
      "Processing batch 400/1563\n",
      "Processing batch 500/1563\n",
      "Processing batch 600/1563\n",
      "Processing batch 700/1563\n",
      "Processing batch 800/1563\n",
      "Processing batch 900/1563\n",
      "Processing batch 1000/1563\n",
      "Processing batch 1100/1563\n",
      "Processing batch 1200/1563\n",
      "Processing batch 1300/1563\n",
      "Processing batch 1400/1563\n",
      "Processing batch 1500/1563\n",
      "Epoch 1/1 -> Train Loss: 0.2656, Train Acc: 0.8900, Test Acc: 0.9269\n",
      "Saved model to models/model_dropout_0.7_epoch_1.pt\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[55], line 21\u001b[0m\n\u001b[1;32m     18\u001b[0m     metrics_dict[dropout] \u001b[38;5;241m=\u001b[39m metrics\n\u001b[1;32m     20\u001b[0m \u001b[38;5;66;03m# plot loss and accuracy\u001b[39;00m\n\u001b[0;32m---> 21\u001b[0m \u001b[43mplot_loss_across_dropouts\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmetrics_dict\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[54], line 108\u001b[0m, in \u001b[0;36mplot_loss_across_dropouts\u001b[0;34m(metrics_dict)\u001b[0m\n\u001b[1;32m    101\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mplot_loss_across_dropouts\u001b[39m(metrics_dict):\n\u001b[1;32m    102\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;124;03m    Plot training loss across different dropout values.\u001b[39;00m\n\u001b[1;32m    104\u001b[0m \n\u001b[1;32m    105\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;124;03m        metrics_dict: Dictionary where keys are dropout values, and values are metrics from training.\u001b[39;00m\n\u001b[1;32m    107\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 108\u001b[0m     fig, ax1 \u001b[38;5;241m=\u001b[39m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39msubplots(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m6\u001b[39m))\n\u001b[1;32m    110\u001b[0m     \u001b[38;5;66;03m# Extract training loss and test accuracy for each dropout\u001b[39;00m\n\u001b[1;32m    111\u001b[0m     dropouts \u001b[38;5;241m=\u001b[39m []\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "lr = 5e-5\n",
    "num_epochs = 1  # Fixed at 1 epoch for this comparison\n",
    "dropout_values = [0.3, 0.5, 0.7]\n",
    "metrics_dict = {}\n",
    "\n",
    "for dropout in dropout_values:\n",
    "    print(f\"Training with lr={lr}, dropout={dropout}\")\n",
    "    metrics = train_with_dropout(\n",
    "        model_class=TransformerBiLSTM,\n",
    "        train_loader=train_loader,\n",
    "        test_loader=test_loader,\n",
    "        num_epochs=num_epochs,\n",
    "        lr=lr,\n",
    "        dropout=dropout,\n",
    "        device=None,\n",
    "        save_dir=\"models\"\n",
    "    )\n",
    "    metrics_dict[dropout] = metrics\n",
    "\n",
    "# plot loss and accuracy\n",
    "plot_loss_across_dropouts(metrics_dict)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training on New Hyperparameters\n",
    "Overall, it seems like dropout fails to improve our model. Hence, we train the full model on 3 epochs with `lr = 5e-05` as it generalises better."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting epoch 1/1\n",
      "Processing batch 1/1563\n",
      "Processing batch 100/1563\n"
     ]
    }
   ],
   "source": [
    "lr = 5e-05\n",
    "metrics = train_model(model, train_loader, test_loader, num_epochs=1, lr=lr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predicting Sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_sentiment(model, tokenizer, text, device=None, max_length=500):\n",
    "    \"\"\"\n",
    "    Predict the sentiment of a given text using the trained model.\n",
    "\n",
    "    Args:\n",
    "        model: Trained TransformerBiLSTM model.\n",
    "        tokenizer: Pretrained tokenizer (e.g., DistilBERT tokenizer).\n",
    "        text: The input text string to classify.\n",
    "        device: Device for computation ('cuda', 'mps', or 'cpu').\n",
    "        max_length: Maximum length for tokenization (default: 500).\n",
    "\n",
    "    Returns:\n",
    "        Predicted sentiment label (e.g., 'positive' or 'negative').\n",
    "    \"\"\"\n",
    "    # Move model to the correct device\n",
    "    if device is None:\n",
    "        device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")  # 'cuda' for Nvidia GPUs, 'mps' for Apple Silicon\n",
    "    model = model.to(device)\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Tokenize and encode the input text\n",
    "    encoded_input = tokenizer(\n",
    "        text,\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=max_length,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    # Move input tensors to the same device as the model\n",
    "    input_ids = encoded_input[\"input_ids\"].to(device)\n",
    "    attention_mask = encoded_input[\"attention_mask\"].to(device)\n",
    "\n",
    "    # Make prediction\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, attention_mask)\n",
    "        predicted_label = outputs.argmax(dim=1).item()  # Get the index of the highest score\n",
    "\n",
    "    # Map predicted label to sentiment class (e.g., 0 -> 'negative', 1 -> 'positive')\n",
    "    sentiment = \"positive\" if predicted_label == 1 else \"negative\"\n",
    "    return sentiment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained model\n",
    "model = TransformerBiLSTM()\n",
    "model.load_state_dict(torch.load(\"my_model_checkpoints/best_model.pt\"))\n",
    "\n",
    "# Define the tokenizer\n",
    "tokenizer = DistilBertTokenizer.from_pretrained(\"distilbert-base-uncased\")\n",
    "\n",
    "# Input text\n",
    "text = \"this movie is so great\"\n",
    "\n",
    "# Predict sentiment\n",
    "device = torch.device(\"cpu\")  # Use 'cuda', 'mps', or 'cpu'\n",
    "sentiment = predict_sentiment(model, tokenizer, text, device=device)\n",
    "print(f\"Predicted Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Input text\n",
    "text = \"this movie is so bad\"\n",
    "\n",
    "# Predict sentiment\n",
    "device = torch.device(\"cpu\")  # Use 'cuda', 'mps', or 'cpu'\n",
    "sentiment = predict_sentiment(model, tokenizer, text, device=device)\n",
    "print(f\"Predicted Sentiment: {sentiment}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Metrics and Saving Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_results(all_sequences, all_labels, all_preds, tokenizer, output_dir=\"results\", zip_filename=\"submission.zip\"):\n",
    "    \"\"\"\n",
    "    Save predictions and true labels to a CSV and zip the results.\n",
    "\n",
    "    Args:\n",
    "        all_sequences: List of input sequences (token IDs).\n",
    "        all_labels: List of true labels.\n",
    "        all_preds: List of predicted labels.\n",
    "        tokenizer: Tokenizer used to decode token IDs into text.\n",
    "        output_dir: Directory to save the results.\n",
    "        zip_filename: Name of the zip file to create.\n",
    "\n",
    "    Returns:\n",
    "        csv_path: Path to the saved CSV file.\n",
    "        zip_path: Path to the saved ZIP file.\n",
    "    \"\"\"\n",
    "    # Decode input sequences back into text using the tokenizer\n",
    "    decoded_sequences = [\n",
    "        tokenizer.decode(seq, skip_special_tokens=True, clean_up_tokenization_spaces=True)\n",
    "        for seq in all_sequences\n",
    "    ]\n",
    "\n",
    "    # Create results DataFrame\n",
    "    results_df = pd.DataFrame({\n",
    "        'sequence': decoded_sequences,\n",
    "        'true_label': all_labels,\n",
    "        'predicted_label': all_preds\n",
    "    })\n",
    "\n",
    "    # Create output directory if it doesn't exist\n",
    "    os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "    # Save to CSV\n",
    "    csv_path = os.path.join(output_dir, \"test_predictions.csv\")\n",
    "    results_df.to_csv(csv_path, index=False)\n",
    "    print(f\"Results saved to {csv_path}\")\n",
    "\n",
    "    # Create a zip file\n",
    "    zip_path = os.path.join(output_dir, zip_filename)\n",
    "    with zipfile.ZipFile(zip_path, 'w') as zipf:\n",
    "        zipf.write(csv_path, arcname=\"test_predictions.csv\")\n",
    "    print(f\"Results zipped to {zip_path}\")\n",
    "\n",
    "    return csv_path, zip_path\n",
    "\n",
    "\n",
    "\n",
    "def cal_metrics(model, test_loader, tokenizer, output_dir=\"results\", zip_filename=\"submission.zip\"):\n",
    "    \"\"\"\n",
    "    Calculate metrics, save predictions to a CSV, and zip the results.\n",
    "\n",
    "    Args:\n",
    "        model: Trained TransformerBiLSTM model.\n",
    "        test_loader: DataLoader for the test set.\n",
    "        tokenizer: Tokenizer used for decoding sequences.\n",
    "        output_dir: Directory to save the results.\n",
    "        zip_filename: Name of the zip file to create.\n",
    "\n",
    "    Returns:\n",
    "        f1: F1-score of the model.\n",
    "        precision: Precision of the model.\n",
    "        recall: Recall of the model.\n",
    "        accuracy: Accuracy of the model.\n",
    "        zip_path: Path to the saved ZIP file.\n",
    "    \"\"\"\n",
    "    device = next(model.parameters()).device  # Get device (e.g., 'cuda', 'mps', 'cpu')\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "\n",
    "    # Initialize lists for predictions, labels, and sequences\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_sequences = []\n",
    "\n",
    "    # Collect predictions and true labels\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            input_ids, attention_mask, labels = batch\n",
    "            input_ids, attention_mask, labels = input_ids.to(device), attention_mask.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(input_ids, attention_mask)\n",
    "            preds = torch.argmax(outputs, dim=1)\n",
    "            all_preds.extend(preds.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            all_sequences.extend(input_ids.cpu().numpy())  # Save raw input sequences\n",
    "\n",
    "    # Calculate metrics\n",
    "    tp = sum(1 for pred, label in zip(all_preds, all_labels) if pred == label == 1)\n",
    "    fp = sum(1 for pred, label in zip(all_preds, all_labels) if pred == 1 and label == 0)\n",
    "    fn = sum(1 for pred, label in zip(all_preds, all_labels) if pred == 0 and label == 1)\n",
    "    tn = sum(1 for pred, label in zip(all_preds, all_labels) if pred == label == 0)\n",
    "\n",
    "    accuracy = sum(1 for pred, label in zip(all_preds, all_labels) if pred == label) / len(all_labels)\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    print(f\"Test Set Metrics:\\nAccuracy: {accuracy:.4f}\\nPrecision: {precision:.4f}\\nRecall: {recall:.4f}\\nF1-Score: {f1:.4f}\")\n",
    "\n",
    "    # Save results using the helper function\n",
    "    csv_path, zip_path = save_results(all_sequences, all_labels, all_preds, tokenizer, output_dir, zip_filename)\n",
    "\n",
    "    return f1, precision, recall, accuracy, zip_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Set Metrics:\n",
      "Accuracy: 0.8585\n",
      "Precision: 0.8783\n",
      "Recall: 0.8323\n",
      "F1-Score: 0.8547\n",
      "Results saved to results/test_predictions.csv\n",
      "Results zipped to results/submission.zip\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "(0.8546783865932802,\n",
       " 0.8782711463785244,\n",
       " 0.83232,\n",
       " 0.85848,\n",
       " 'results/submission.zip')"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Evaluate the trained model and save results\n",
    "f1, precision, recall, accuracy, zip_path = cal_metrics(\n",
    "    model=model, \n",
    "    test_loader=test_loader, \n",
    "    tokenizer=tokenizer, \n",
    "    output_dir=\"results\", \n",
    "    zip_filename=\"submission.zip\"\n",
    ")\n",
    "\n",
    "print(f\"Saved results to {zip_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "mini project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
